{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrikAI - The CookAI monster !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jfcaro/anaconda3/lib/python3.7/site-packages/google/colab/data_table.py:30: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  from IPython.utils import traitlets as _traitlets\n"
     ]
    }
   ],
   "source": [
    "# modules for storing and plotting the information\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "\n",
    "# modules for web app\n",
    "from flask import Flask, render_template,flash, redirect\n",
    "from flask import request\n",
    "from flask import url_for\n",
    "from forms import Video_playlistForm, Consult\n",
    "import requests\n",
    "\n",
    "# modules for dAIogenes (web-scrapping youtube videos)\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from pytube import Playlist\n",
    "import os \n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "\n",
    "# modules for TrikAI (to synthetize texts)\n",
    "\n",
    "# For clustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# For topic extraction\n",
    "import ktrain\n",
    "\n",
    "# For level detection and doc_to_vect function\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of variables\n",
    "path_Wordcloud = \"./static/Wordcloud/\"\n",
    "path = \"./\"\n",
    "path_figures = \"./static/Figures/\"\n",
    "path_models = \"./static/Models/\"\n",
    "path_temp = \"./static/Temp/\"\n",
    "\n",
    "\n",
    "\n",
    "# Functions\n",
    "\n",
    "#####################################################################################################################################\n",
    "# WEBAPP\n",
    "\n",
    "# Provide head for webapp pages\n",
    "def head(title):\n",
    "    \n",
    "    return \"\"\"<center><table class=\"default\">\n",
    "    <td><center><a href=\\\"\"\"\"+ url_for('index')+\"\"\"\"\\\"><img src=\"\"\"+url_for('static', filename=\"Arts/CookAI_monster_mini.png\")+\"\"\"></center></td>\n",
    "    <td><center><h1>\"\"\"+ str(title) +\"\"\"</h1></center></td>\n",
    "    </table>\"\"\"\n",
    "\n",
    "#####################################################################################################################################\n",
    "# DAIOGENES\n",
    "\n",
    "# Provide transcrip from a YouTube link\n",
    "def get_transcript (link):\n",
    "  id = link.split(\"https://www.youtube.com/watch?v=\")[1]\n",
    "  transcripcion = YouTubeTranscriptApi.get_transcript(id)\n",
    "  transcript = \"\"\n",
    "  for i in range(0,len(transcripcion)):\n",
    "    transcript +=str(transcripcion[i]['text'])+\" \"\n",
    "  return transcript  \n",
    "\n",
    "# Provide name of the playlist and list of urls from the videos that belongs to the playlist\n",
    "def get_playlist (link):\n",
    "    try: \n",
    "        playlist = Playlist(link)\n",
    "        name = playlist.title\n",
    "    except: \n",
    "        playlist =[]\n",
    "        name =\"Error\"\n",
    "    return name, playlist\n",
    "\n",
    "# Gives the lenght in words of a document\n",
    "def document_size(doc):\n",
    "    return len(doc)\n",
    "\n",
    "# provides metadata from a YouTube video\n",
    "def import_video_data(URL):\n",
    "    page_source = requests.get(URL)\n",
    "    page_source = page_source.text\n",
    "    try: \n",
    "      title = page_source[page_source.find(\"<meta name=\\\"title\\\" content=\\\"\") :page_source.find(\"<meta name=\\\"title\\\" content=\\\"\")+200].split(\"<meta name=\\\"title\\\" content=\\\"\")[1].split(\"\\\">\")[0]     \n",
    "    except: \n",
    "      title = \"None\"\n",
    "    try: \n",
    "      rating = page_source[page_source.find(\"\\\"averageRating\\\":\"): page_source.find(\"averageRating\")+200].split(\"\\\"averageRating\\\":\")[1].split(\",\\\"allowRatings\\\"\")[0]\n",
    "    except:\n",
    "      rating = \"None\"\n",
    "    try: \n",
    "      views = page_source[page_source.find(\"\\\"viewCount\\\":\"): page_source.find(\"\\\"viewCount\\\":\")+200].split(\"\\\"viewCount\\\":\")[1].split(\",\\\"author\\\"\")[0].replace(\"\\\"\",\"\")\n",
    "    except:\n",
    "      views = \"None\"\n",
    "    try: \n",
    "      author = page_source[page_source.find(\"\\\"author\\\":\"): page_source.find(\"\\\"author\\\":\")+200].split(\"\\\"author\\\":\")[1].split(\",\\\"isPrivate\\\"\")[0].replace(\"\\\"\",\"\")\n",
    "    except:\n",
    "      author = \"None\"\n",
    "    sleep(randint(2,7))\n",
    "    return URL, title, rating, views, author\n",
    "\n",
    "\n",
    "#####################################################################################################################################\n",
    "# STORING AND PLOTTING\n",
    "\n",
    "# creation of png file with wordcloud\n",
    "def gen_wordcloud(transcription,index):\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS, background_color='white', max_words=100, width=853, height=300).generate(transcription)\n",
    "    wordcloud.to_file(path_Wordcloud +str(index)+\".png\")\n",
    "    \n",
    "# creation of png file with wordcloud for the temp folder    \n",
    "def gen_wordcloud_temp(transcription,index):\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS, background_color='white', max_words=100, width=560, height=150).generate(transcription)\n",
    "    wordcloud.to_file(path_temp+str(index)+\".png\")\n",
    "    return \"Temp/\"+ str(index)+\".png\"\n",
    "\n",
    "\n",
    "# visualize table \n",
    "def render_table(df, title=\"\"):\n",
    "    table = \"\"\"<center><h3>\"\"\"+str(title)+\"\"\"</h3></center>\"\"\"\n",
    "    table +=\"\"\"<center><table class= \\\"default\\\">\n",
    "    <tr>\n",
    "    <th><center>Author</center></th>\n",
    "    <th><center>Link</center></th>\n",
    "    <th><center>Rating</center></th>\n",
    "    <th><center>Level</center></th>\"\"\"\n",
    "    \n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        table+=\"<tr><td><a href=\\\"\"+ url_for('query', column =\"author\", value = row[\"author\"])+\"\\\">\"+row[\"author\"]+\"</a></td><td><a href=\\\"\"+ url_for('params', video_id= i)+\"\\\">\"+row[\"title\"]+\"</a></td><td>\"+ str(row[\"rating\"])+\"</td><td>\"+ str(row[\"level\"])+\"</td>\"\n",
    "        \n",
    "        \n",
    "    table +=\"</table></center>\"\n",
    "    return table\n",
    "\n",
    "# render in html a generic table  ---> ME PARECE QUE NO LA USO \n",
    "def render_table_generic(df):\n",
    "    columns = df.columns\n",
    "    table =\"\"\"<center><table class= \\\"default\\\"><tr>\"\"\"\n",
    "    for c in columns: \n",
    "        table+= \"<th>\"+c+\"</th>\"\n",
    "    for i, row in df.iterrows():\n",
    "        table +=\"<tr>\"\n",
    "        for c in columns:\n",
    "            table+=\"<td>\"+str(row[c])+\"</td>\"\n",
    "        table +=\"<tr>\"\n",
    "    table +=\"</table></center>\"\n",
    "    return table \n",
    " \n",
    "# render in html a generic table with links    \n",
    "def render_table_generic_links(df,title, cols=[]):\n",
    "    if cols !=[]:\n",
    "        cquery = cols[0]\n",
    "        cvalue=cols[1]        \n",
    "        df[str(cquery)+\" links\"]=df.apply(lambda x: \"\"\"<a href= \"\"\" +\"\\\"\"+ url_for('query',column=cquery,value =int(x[cvalue]))+\"\\\"\"+\"\"\"\">\"\"\"+x[cquery]+\"\"\"</a>\"\"\", axis = 1)\n",
    "        df.drop(cols, axis=1, inplace=True) \n",
    "       \n",
    "        \n",
    "    columns = df.columns\n",
    "    table = \"<center><h3>\"+str(title)+\"</h3></center>\"\n",
    "    table +=\"\"\"<center><table class= \\\"default\\\"><tr>\"\"\"\n",
    "    for c in columns: \n",
    "        table+= \"<th><center>\"+c+\"</center></th>\"\n",
    "    for i, row in df.iterrows():\n",
    "        table +=\"<tr>\"\n",
    "        for c in columns:\n",
    "            table+=\"<td>\"+str(row[c])+\"</td>\"\n",
    "        table +=\"<tr>\"\n",
    "    table +=\"</table></center>\"\n",
    "    return table  \n",
    "\n",
    "\n",
    "#####################################################################################################################################\n",
    "# TRIKAI-CLUSTERING\n",
    "\n",
    "# returns the cluster of a set of keywords\n",
    "def cluster_keywords (keyword):\n",
    "    vectorizer = pickle.load(open(path_models+\"tfidf.pickle\", 'rb'))\n",
    "    #vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    Y = vectorizer.transform([keyword])\n",
    "    model = pickle.load(open(path_models +\"cluster.pickle\", 'rb'))\n",
    "    prediction = model.predict(Y)\n",
    "    # print (\"Cluster number\",prediction[0],\"\\n\")\n",
    "    return prediction[0]\n",
    "\n",
    "\n",
    "# returns the centroids of a cluster given the model and the vectorizer\n",
    "def cluster_centroids (model,vectorizer):\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    n_clusters = model.n_clusters\n",
    "    terms =vectorizer.get_feature_names()\n",
    "    centroid={}\n",
    "    for cent in range(n_clusters):    \n",
    "        words =[]\n",
    "        for ind in order_centroids[cent, :10]:\n",
    "            words.append(terms[ind])\n",
    "        words_string = \" \".join(words)\n",
    "        centroid[cent] =words_string    \n",
    "    return centroid\n",
    "\n",
    "\n",
    "#####################################################################################################################################\n",
    "# TRIKAI-TOPIC EXTRACTION\n",
    "\n",
    "# load topic model \n",
    "def load_topic_model(fname):\n",
    "\n",
    "    with open(fname+'.tm_vect', 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    with open(fname+'.tm_model', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    with open(fname+'.tm_params', 'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "    tm = ktrain.text.get_topic_model(n_topics=params['n_topics'],\n",
    "                         n_features = params['n_features'],\n",
    "                         verbose = params['verbose'])\n",
    "    tm.model = model\n",
    "    tm.vectorizer = vectorizer\n",
    "    return tm\n",
    "\n",
    "\n",
    "# topic identification from pretrained model \n",
    "def topic_identification(text):\n",
    "   \n",
    "    # Load the model pretrained\n",
    "    \n",
    "    fname =(path_models +\"topic\")\n",
    "    \n",
    "    with open(fname+'.tm_vect', 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    with open(fname+'.tm_model', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    with open(fname+'.tm_params', 'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "    tm = ktrain.text.get_topic_model(n_topics=params['n_topics'],\n",
    "                         n_features = params['n_features'],\n",
    "                         verbose = params['verbose'])\n",
    "    tm.model = model\n",
    "    tm.vectorizer = vectorizer\n",
    "\n",
    "    return tm.predict([text]).argmax()\n",
    "\n",
    "\n",
    "#####################################################################################################################################\n",
    "# TRIKAI-LEVEL DETECTION AND DOC_TO_VECT\n",
    "\n",
    "# load en_core_web_lg nlp model \n",
    "nlp =spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "\n",
    "# returns the vector of a transcript based on spacy model en_core_web_lg\n",
    "def doc_to_vect(transcript):\n",
    "  doc = nlp(transcript)\n",
    "  return doc.vector\n",
    "\n",
    "# returns the level of difficulty (basic/advanced) of a AI transcript\n",
    "def predict_level(nlp, texts): \n",
    "    # Use the model's tokenizer to tokenize each input text\n",
    "    docs = [nlp.tokenizer(text) for text in texts]\n",
    "    \n",
    "    # Use textcat to get the scores for each doc\n",
    "    textcat = nlp.get_pipe('textcat')\n",
    "    scores = textcat.predict(docs)\n",
    "    \n",
    "    # From the scores, find the class with the highest score/probability\n",
    "    predicted_class =  scores.argmax(axis=1)\n",
    "    \n",
    "    if predicted_class == 0:\n",
    "        predicted_class_str =\"basic\"\n",
    "    else: \n",
    "        predicted_class_str =\"advanced\"\n",
    "    \n",
    "    return predicted_class_str\n",
    "\n",
    "\n",
    "#####################################################################################################################################\n",
    "# TRIKAI-CORE-SEARCHER\n",
    "\n",
    "# search function, provides a video_id\n",
    "def search(consult_text,level):\n",
    "    \n",
    "    df = pd.read_csv(\"youtube_transcrip_vector.csv\") \n",
    "    df.astype({\"len\": int, \"views\":int, \"Cluster\":int, \"topic\":int })\n",
    "    df[\"title_agg\"] = df[['title', 'author', 'type', 'playlist_name']].agg(' '.join, axis=1).apply(str.lower)\n",
    "    \n",
    "    levels =[]\n",
    "        \n",
    "    if level ==\"B\":\n",
    "        levels =  [\"basic\"]\n",
    "    elif level ==\"A\": \n",
    "        levels = [\"intermediate\",\"advanced\"]\n",
    "    else:\n",
    "        levels =[\"basic\",\"intermediate\",\"advanced\"]\n",
    "        \n",
    "    #print (\"Level:\" +str(levels))\n",
    "        \n",
    "    consult_len = len(consult_text.split())\n",
    "        \n",
    "    #print(\"Consult_len:\"+str(consult_len))\n",
    " \n",
    "    # to provide the best video_id including searchs in title,author, type, playlist name, cluster and topics    \n",
    "    video_idtit = set()\n",
    "        \n",
    "    if consult_len <=3:            \n",
    "        consult_list = consult_text.lower().split()\n",
    "        for i in range(consult_len-1,3):\n",
    "            consult_list.append(\"\")\n",
    "        title = df[df[\"level\"].isin(levels)][\"title_agg\"].apply(lambda x: (consult_list[0] in x )&(consult_list[1] in x)&(consult_list[2] in x))\n",
    "        video_idtit = set(title[title ==True].index.tolist())\n",
    "                 \n",
    " \n",
    "    cluster = cluster_keywords(consult_text)        \n",
    "    video_idc =set (df[(df[\"Cluster\"]==cluster)&(df[\"level\"].isin(levels))].index.tolist())\n",
    "    #print (\"Cluster:\"+str(cluster))\n",
    "        \n",
    "    topic = topic_identification(consult_text)    \n",
    "    video_idt = set(df[(df[\"topic\"]==topic)&(df[\"level\"].isin(levels))].index.tolist())\n",
    "    #print (\"Topic:\"+str(topic))\n",
    "    #print (\"video_idtit:\"+str(video_idtit))\n",
    "    #print (\"video_cluster:\"+str(video_idc))\n",
    "    #print (\"video_topic:\"+ str(video_idt))\n",
    "    \n",
    "    if video_idtit != set():\n",
    "        \n",
    "        if video_idc & video_idt & video_idtit != set():\n",
    "            video_ids = list((video_idc & video_idt & video_idtit))\n",
    "            \n",
    "            \n",
    "        elif video_idt & video_idtit != set():\n",
    "            video_ids = list((video_idt & video_idtit))                \n",
    "            \n",
    "        else: \n",
    "            video_ids = list(video_idtit)                \n",
    "    else: \n",
    "            \n",
    "        if video_idc & video_idt != set():\n",
    "            video_ids = list((video_idc & video_idt))                \n",
    "            \n",
    "        elif video_idt !=set(): \n",
    "            video_ids = list(video_idt)\n",
    "        \n",
    "        else:\n",
    "            video_ids = list(video_idc)\n",
    "        \n",
    "    \n",
    "    #print (\"video_ids:\"+str(video_ids))\n",
    "    video_sel = df.iloc[video_ids][df[\"rating\"]!=\"None\"]\n",
    "    \n",
    "    if video_sel.shape[0]>0: \n",
    "        video_id = video_sel.sort_values('rating',ascending=False).index[0]\n",
    "    \n",
    "    else:\n",
    "        video_id =video_sel.index[0]\n",
    "    \n",
    "    \n",
    "    return video_id      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################################\n",
    "# FLASK WEBAPP\n",
    "\n",
    "\n",
    "app = Flask(__name__ )\n",
    "\n",
    "app.secret_key = os.getenv('SECRET_KEY', 'secret string')\n",
    "\n",
    "\n",
    "@app.route('/',methods=['GET', 'POST'])\n",
    "\n",
    "# Index page : shows a form and links inside de webapp\n",
    "def index():\n",
    "    form = Consult()\n",
    "    if request.method == \"POST\":\n",
    "        consult_text = form.consult.data\n",
    "        level = form.level.data \n",
    "        video_id = search(consult_text,level)\n",
    "        return redirect(url_for('params', video_id= video_id))\n",
    "    return render_template('index.html', form=form)    \n",
    "\n",
    " \n",
    "@app.route('/params')\n",
    "\n",
    "# params page : shows the video selected, worcloud and related videos\n",
    "def params():\n",
    "    param = request.args.get(\"video_id\", \"None\")\n",
    "    dfy = pd.read_csv(path+\"youtube_transcrip_vector.csv\") \n",
    "    i= int(param)    \n",
    "    file = 'Wordcloud/'+str(i)+'.png'    \n",
    "    df = dfy.iloc[i]    \n",
    "    \n",
    "    # Get table of topics and centroids of the clusters\n",
    "    topics = pickle.load(open(path_models + \"topics.pickle\", 'rb'))\n",
    "    centroids = pickle.load(open(path_models + \"centroids.pickle\", 'rb'))\n",
    "    \n",
    "    # Calculate related videos\n",
    "    df_topic = dfy[(dfy[\"topic\"]==df.topic)&(dfy[\"rating\"]!=\"None\")].sort_values('rating',ascending=False)[[\"title\", \"link\", \"author\",\"playlist_name\",\"rating\",\"level\"]].drop_duplicates(subset =\"playlist_name\")\n",
    "    df_cluster = dfy[(dfy[\"Cluster\"]==df.Cluster)&(dfy[\"rating\"]!=\"None\")].sort_values('rating',ascending=False)[[\"title\", \"link\", \"author\",\"playlist_name\",\"rating\",\"level\"]].drop_duplicates(subset =\"playlist_name\")\n",
    "    \n",
    "    similars = docvectors = pickle.load(open(path_models + \"doc_similars.pickle\", 'rb'))  \n",
    "    df_similars = dfy.iloc[list(similars[i][-7:-1][::-1])]\n",
    "    \n",
    "    table1=render_table(df_topic.head(6),\"Selection in the same TOPIC\")\n",
    "    table2=render_table(df_cluster.head(6),\"Selection in the same CLUSTER\")\n",
    "    table3=render_table(df_similars, \"Selection similar CONTENT\")\n",
    "    \n",
    "    return \"\"\"\n",
    "    <center><table class=\"default\">\n",
    "    <td><center><a href=\\\"\"\"\"+ url_for('index')+\"\"\"\"\\\"><img src=\"\"\"+url_for('static', filename=\"Arts/CookAI_monster_mini.png\")+\"\"\"></center></td>\n",
    "    <td><center><h1>TrikAI recommends you this special content...</h1></center></td>\n",
    "    </table>\n",
    "    <br>   \n",
    "    <h2>Title: \"\"\"+df.title+\"\"\"</h2>    \n",
    "    <center><h3><b>Playlist:</b><a href=\"\"\"+ \"\\\"\" + url_for('playlist', playlist_name= df.playlist_name)+\"\\\"\"+\"\"\"\">\"\"\"+df.playlist_name+\"\"\"</a><h3> </center>\n",
    "    <table class=\"default\">\n",
    "    <tr>\n",
    "    <th><center><h3><b>Author</b>:<a href= \"\"\" +\"\\\"\"+ url_for('query',column='author',value =df.author)+\"\\\"\"+\"\"\"\">\"\"\"+df.author+\"\"\"</a></h3></center></th>\n",
    "    <th><center><h3><b>Views</b>: \"\"\"+str(df.views)+\"\"\"</h3></center></th>\n",
    "    <th><center><h3><b>Rating:</b> \"\"\"+str(df.rating)+\"\"\"</h3></center></th>    \n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td><center><h3>Level: \"\"\"+str(df.level)+\"\"\"</h3></center></td>       \n",
    "    <td><center><h3>Length: \"\"\"+str(df.len)+\"\"\"</h3></center></td>\n",
    "    <td><center><h3>Type: \"\"\"+df.type+\"\"\"</h3></center></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td colspan=\"3\"><center><h4>Topics: \"\"\"+topics[int(df.topic)]+\"\"\"<h4></center></td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td colspan=\"3\"><center><h4>Cluster: \"\"\"+centroids[int(df.Cluster)]+\"\"\"<h4></center></td>\n",
    "    </tr> \n",
    "    </table>\n",
    "    <br>\n",
    "    <center><table class=\"default\"><td>\n",
    "    <a href=\\\"\"\"\"+ url_for('params', video_id = i-1)+\"\"\"\"\\\"><img src=\"\"\"+url_for('static', filename=\"Arts/arrow_decrease.png\")+\"\"\"></a>\n",
    "    <iframe src=\"https://www.youtube.com/embed/\"\"\"+df[2].split(\"watch?v=\")[1]+\"\"\"\" width=\"853\" height=\"480\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "    <a href=\\\"\"\"\"+ url_for('params', video_id = i+1)+\"\"\"\"\\\"><img src=\"\"\"+url_for('static', filename=\"Arts/arrow_increase.png\")+\"\"\"></a></td>\n",
    "    </table></center>\n",
    "    <br>    \n",
    "    <a href=\\\"\"\"\"+ url_for('show', video_id = i)+\"\"\"\"\\\"><img src=\"\"\"+url_for('static', filename=file)+\"\"\"></a></td>   \n",
    "    <br>\n",
    "    <br>\n",
    "    <br>\n",
    "    \"\"\" + table1+ \"\"\"<br><br>\"\"\"+table2+ \"\"\"<br><br>\"\"\"+table3\n",
    "\n",
    "\n",
    "@app.route('/load_url')\n",
    "\n",
    "# load url page : get the information of a YouTuble video list\n",
    "def load_url():\n",
    "    \n",
    "    p = request.args.get(\"link\",\"None\") \n",
    "    dtype = str(request.args.get(\"dtype\",\"None\"))\n",
    "    p_name, p_list = get_playlist(p)\n",
    "    \n",
    "    # model for the category\n",
    "    nlp_cat = spacy.load(path_models+\"nlp\")\n",
    "    \n",
    "    videos = pd.DataFrame({\"link\":[], \"transcript\":[], \"playlist_name\":[],\"playlist_link\":[], \"level\":[], \"type\":[]})\n",
    "    \n",
    "    #print (p_name)\n",
    "    # all the urls of the playlist\n",
    "    for url in p_list:        \n",
    "        try: \n",
    "            transcripcion = get_transcript(url)    \n",
    "            level = predict_level(nlp_cat, [transcripcion,])\n",
    "            #print (url)\n",
    "            #print (level)            \n",
    "            videos=videos.append({\"link\": url,\"transcript\": transcripcion, \"playlist_name\":p_name, \"playlist_link\":p, \"level\":level, \"type\":dtype} , ignore_index=True)\n",
    "        except:\n",
    "            print (\"Problem!!!\")\n",
    "        \n",
    "        sleep(randint(0,5))\n",
    "        #print(\"\\n\")\n",
    "        \n",
    "    # manage the metadata\n",
    "    metadata = pd.DataFrame({\"link\":[],\"title\":[], \"rating\":[], \"views\":[], \"author\":[]})\n",
    "    \n",
    "    # scrapping all the videos\n",
    "    for i in range(0,videos.shape[0]):    \n",
    "        meta = import_video_data(videos[\"link\"].iloc[i])\n",
    "        metadata=metadata.append({\"link\": meta[0],\"title\":meta[1], \"rating\":meta[2], \"views\":meta[3], \"author\":meta[4] } , ignore_index=True)\n",
    "        if (i % 100 ==0)&(i !=0):\n",
    "            #print (i)\n",
    "            #print (meta)\n",
    "            sleep(randint(10,15)) \n",
    "    \n",
    "    # merge videos and metadata dataframes\n",
    "    total = pd.merge(videos,metadata,left_on='link', right_on='link', how ='left')\n",
    "    \n",
    "    # Len column\n",
    "    total[\"len\"]= total[\"transcript\"].apply(document_size)\n",
    "    \n",
    "    # Cluster column\n",
    "    total[\"Cluster\"]= total[\"transcript\"].apply(cluster_keywords)   \n",
    "    centroids = pickle.load(open(path_models + \"centroids.pickle\", 'rb'))\n",
    "    \n",
    "    # Topic column\n",
    "    topics = pickle.load(open(path_models + \"topics.pickle\", 'rb'))\n",
    "    total[\"topic\"]= total[\"transcript\"].apply(topic_identification)         \n",
    "     \n",
    "    # reindex datafrae    \n",
    "    total = total.reindex(columns=['title','author','link','len','transcript','playlist_name','playlist_link','rating','views', 'type','level','vector','Cluster', 'topic'])\n",
    "    \n",
    "    # stores in temp directory   \n",
    "    if os.path.exists(path_temp+\"daiogenes.csv\"):        \n",
    "        daiogenes = pd.read_csv(path_temp+ \"daiogenes.csv\")    \n",
    "        pd.concat([daiogenes, total], axis=0,ignore_index=True).to_csv(path_temp+\"daiogenes.csv\",index=False)        \n",
    "    else:\n",
    "        total.to_csv(path_temp+\"daiogenes.csv\", index=False)\n",
    "    \n",
    "    # render the output\n",
    "    code = \"\"    \n",
    "    for i in range(0, total.shape[0]):\n",
    "         file = gen_wordcloud_temp(total.iloc[i]['transcript'],i)        \n",
    "    \n",
    "         code+= \"<br><center>\"+\"<b>Title</b>:\"+total.iloc[i]['title']+\" |-----| \"+\"<b>Level</b>:\"+total.iloc[i]['level']+\"</center><br><center>\"+\"<b>Topic</b>:\"+str(topics[total.iloc[i]['topic']])+\" <br>\"+\"<b>Cluster</b>:\"+str(centroids[total.iloc[i]['Cluster']])+\"</center><br><center><iframe src=\\\"https://www.youtube.com/embed/\"+total.iloc[i][\"link\"].split(\"watch?v=\")[1]+\"\\\"\"+\"width=\\\"560\\\" height=\\\"315\\\" frameborder=\\\"0\\\" allowfullscreen></iframe></center><br><img src=\"+url_for('static', filename=file)+\"><br><br>\"   \n",
    "         \n",
    "    return \"\"\"\n",
    "    <center><table class=\"default\">\n",
    "    <td><center><a href=\\\"\"\"\"+ url_for('index')+\"\"\"\"\\\"><img src=\"\"\"+url_for('static', filename=\"Arts/CookAI_monster_mini.png\")+\"\"\"></center></td>\n",
    "    <td><center><h1>Playlist Digested !</h1></center></td>\n",
    "    </table>\n",
    "    <br>\n",
    "    <br><h2>\"\"\"+p_name+\"</h2>\"+str(code)  \n",
    "    \"\"\"\n",
    "    </center>\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "@app.route('/library')\n",
    "\n",
    "# library page : shows relevant information of the dataframe\n",
    "def library():\n",
    "    \n",
    "    df = pd.read_csv(\"youtube_transcrip_vector.csv\")\n",
    "    \n",
    "    # total num of videos and total words\n",
    "    num_videos = df.shape[0]\n",
    "    total_length = df['len'].sum()\n",
    "    \n",
    "    # histogram of lengths\n",
    "    fig, ax = plt.subplots(figsize=(5, 5), dpi=70)\n",
    "    sns.histplot(data=df['len'])\n",
    "    fig.savefig(path_figures +'len_fig.png', dpi =70)\n",
    "    file_len = 'Figures/len_fig.png'   \n",
    "    \n",
    "    # histogram of levels\n",
    "    fig, ax = plt.subplots(figsize=(5, 5), dpi=70)\n",
    "    sns.histplot(data=df['level'])\n",
    "    fig.savefig(path_figures +'level_fig.png', dpi =70)\n",
    "    file_level = 'Figures/level_fig.png' \n",
    "    \n",
    "    # histogram of types\n",
    "    fig, ax = plt.subplots(figsize=(5, 5), dpi=70)\n",
    "    sns.histplot(data=df['type'])\n",
    "    fig.savefig(path_figures +'type_fig.png', dpi =70)\n",
    "    file_type = 'Figures/type_fig.png' \n",
    "    \n",
    "    # topic table calculation\n",
    "    counts =df[\"topic\"].value_counts()\n",
    "    data = pd.DataFrame({\"index\":counts.index, \"counts\":counts.values})\n",
    "    topics = pickle.load(open(path_models + \"topics.pickle\", 'rb'))\n",
    "    data[\"topic\"]=data[\"index\"].astype(int).apply(lambda x: topics[x])\n",
    "    \n",
    "    # cluster table calculations\n",
    "    cluster_counts =df[\"Cluster\"].value_counts()\n",
    "    data_c = pd.DataFrame({\"index\":cluster_counts.index, \"counts\":cluster_counts.values})\n",
    "    centroids = pickle.load(open(path_models + \"centroids.pickle\", 'rb'))\n",
    "    data_c[\"Cluster\"]=data_c[\"index\"].astype(int).apply(lambda x: centroids[x])\n",
    "    \n",
    "    \n",
    "    return head(\"TrikAI library\")+\"\"\"    \n",
    "    <center><h2>Total Videos: \"\"\"+str(num_videos)+\"\"\"</h2></center>\n",
    "    <center><h2>Total Length: \"\"\"+str(int(total_length))+\"\"\"</h2></center>\n",
    "    <br>\n",
    "    <br>\n",
    "    <br>\n",
    "    <table class=\"default\">\n",
    "    <tr>\n",
    "    <th><center><h2>Length</h2></center></th>\n",
    "    <th><center><h2>Level</h2></center></th>\n",
    "    <th><center><h2>Type</h2></center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td><center><img src=\"\"\"+url_for('static', filename=file_len)+\"\"\"></center></td> \n",
    "    <td><center><img src=\"\"\"+url_for('static', filename=file_level)+\"\"\"></center></td>      \n",
    "    <td><center><img src=\"\"\"+url_for('static', filename=file_type)+\"\"\"></center></td>  \n",
    "    </table>  \n",
    "    <br>\n",
    "    \"\"\" + render_table_generic_links(data_c,\"Clusters and number of videos\", cols=['Cluster','index'])+\"\"\"<br><br>\"\"\"+ render_table_generic_links(data,\"Topics and number of videos\", cols=['topic','index'])\n",
    "\n",
    "\n",
    "@app.route('/dAIogenes', methods=['GET', 'POST'])\n",
    "\n",
    "# dAIogenes page : form to get valid url playlist\n",
    "def dAIogenes():\n",
    "    form = Video_playlistForm()\n",
    "    \n",
    "    if request.method == \"POST\":\n",
    "        #print (form.url.data)\n",
    "        return redirect(url_for('load_url', link = form.url.data, dtype = form.playlist_type.data))    \n",
    "      \n",
    "    return render_template('url.html', form=form)\n",
    "\n",
    "\n",
    "@app.route('/cluster')\n",
    "\n",
    "# cluster page : to calculate the clusters from the transcripts\n",
    "def cluster():   \n",
    "\n",
    "    def cluster_document(doc):\n",
    "        Y = vectorizer.transform([doc])\n",
    "        prediction = model.predict(Y)\n",
    "        return prediction[0]\n",
    "\n",
    "    documents =[]\n",
    "    df = pd.read_csv(\"youtube_transcrip_vector.csv\")\n",
    "    \n",
    "    for i in range(0,df.shape[0]):\n",
    "       documents.append(df.iloc[i]['transcript'])\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "\n",
    "    # Manual number of clusters !!!!\n",
    "    # --------------------------------------------\n",
    "    true_k = 30\n",
    "    # --------------------------------------------\n",
    "\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=200, n_init=1)\n",
    "    model.fit(X)\n",
    "   \n",
    "    df['Cluster']=df['transcript'].apply(cluster_document)\n",
    "    \n",
    "    df.to_csv(\"youtube_transcrip_vector.csv\",index=False)\n",
    "    \n",
    "    centroids = cluster_centroids(model, vectorizer)\n",
    "    \n",
    "    \n",
    "    # save the model \n",
    "    #print (\"save\")\n",
    "    pickle.dump(centroids, open(path_models+\"centroids.pickle\", 'wb'))\n",
    "    pickle.dump(model, open(path_models+\"cluster.pickle\", 'wb'))\n",
    "    pickle.dump(vectorizer, open(path_models+\"tfidf.pickle\", \"wb\"))\n",
    "    \n",
    "    # cluster summary    \n",
    "    cluster_counts =df[\"Cluster\"].value_counts()\n",
    "    data_c = pd.DataFrame({\"index\":cluster_counts.index, \"counts\":cluster_counts.values})\n",
    "    centroids = pickle.load(open(path_models + \"centroids.pickle\", 'rb'))\n",
    "    data_c[\"Cluster\"]=data_c[\"index\"].apply(lambda x: centroids[x])\n",
    "    \n",
    "    return head(\"Cluster Generation done! \") + render_table_generic_links(data_c,\"Clusters\", cols=['Cluster','index'])\n",
    "\n",
    "\n",
    "@app.route('/playlist')\n",
    "\n",
    "# playlist page: to show a playlist given as argument\n",
    "def playlist():  \n",
    "    playlist_name = request.args.get(\"playlist_name\", \"None\")\n",
    "    df = pd.read_csv(path+\"youtube_transcrip_vector.csv\")    \n",
    "    df_playlist = df[df[\"playlist_name\"]==playlist_name].sort_index(ascending=True)[[\"title\", \"link\", \"author\",\"rating\",\"level\"]]\n",
    "        \n",
    "    table=render_table(df_playlist)         \n",
    "\n",
    "    return  head(\"Playlist details:\") + table\n",
    "\n",
    "\n",
    "@app.route('/query')\n",
    "\n",
    "# query page: to show a table for the query given as argument\n",
    "def query():\n",
    "    query = request.args.to_dict()\n",
    "    #print (query)\n",
    "    column= query[\"column\"]\n",
    "    value = query[\"value\"]\n",
    "    if value.isnumeric():\n",
    "        value = int(value)\n",
    "    df = pd.read_csv(path+\"youtube_transcrip_vector.csv\")    \n",
    "    \n",
    "    df_query = df[df[column]==value].sort_index(ascending=True)[[\"title\", \"link\", \"author\",\"rating\",\"level\"]]\n",
    "        \n",
    "    table=render_table(df_query, str(column)+\" with value: \"+str(value))   \n",
    "    \n",
    "    return head(\"TrikAI query\") + table \n",
    "\n",
    "\n",
    "@app.route('/topic')\n",
    "\n",
    "# topic page: to calculate the topics from the transcripts\n",
    "def topic():  \n",
    "    \n",
    "    def topic_document(text):\n",
    "        prediction = tm.predict([text]).argmax()        \n",
    "        return prediction    \n",
    "      \n",
    "    df = pd.read_csv(\"youtube_transcrip_vector.csv\")\n",
    "    \n",
    "    text = df[\"transcript\"].to_list()\n",
    "    \n",
    "    tm = ktrain.text.get_topic_model(text, n_features =1000)   \n",
    " \n",
    "    tm.build (text, threshold = 0.25)\n",
    "    \n",
    "    tm.print_topics(show_counts = True)\n",
    "    \n",
    "    tm.save(path_models +\"topic\")\n",
    "    \n",
    "    df['topic']=df['transcript'].apply(topic_document)\n",
    "    \n",
    "    df.to_csv(\"youtube_transcrip_vector.csv\",index=False)\n",
    "    \n",
    "    pickle.dump(tm.topics, open(path_models + \"topics.pickle\",'wb'))\n",
    "    \n",
    "    counts =df[\"topic\"].value_counts()\n",
    "    data = pd.DataFrame({\"index\":counts.index, \"counts\":counts.values})\n",
    "    topics = pickle.load(open(path_models + \"topics.pickle\", 'rb'))\n",
    "    data[\"topic\"]=data[\"index\"].apply(lambda x: topics[x])           \n",
    "\n",
    "    return head(\"Topic Generation done!\") + render_table_generic_links(data,\"Topics\", cols=['topic','index'])\n",
    "\n",
    "\n",
    "@app.route('/update')\n",
    "\n",
    "# update page: to include the temporal dataset (daiogenes.csv) in the general dataset (youtube_transcrip_vector.csv), including cluster, topic and vector calculations\n",
    "def update():  \n",
    "    \n",
    "    if os.path.exists(path_temp+\"daiogenes.csv\"):    \n",
    "        df = pd.read_csv(path_temp+\"daiogenes.csv\")\n",
    "        df['vector'] = df['transcript'].apply(doc_to_vect)\n",
    "        df1 = pd.read_csv(path+\"youtube_transcrip_vector.csv\")\n",
    "        last = df1.shape[0]    \n",
    "        df3 = pd.concat([df1, df], axis=0,ignore_index=True)\n",
    "        df3.drop_duplicates(subset=['link'], keep='first', inplace=True, ignore_index=True)\n",
    "        df3.to_csv(path+\"youtube_transcrip_vector_backup.csv\", index=False)\n",
    "        df3.to_csv(path+\"youtube_transcrip_vector.csv\", index=False)\n",
    "        df3_last = df3.shape[0]\n",
    "        tempdocvectors = []\n",
    "        for i in range(last, df3_last):\n",
    "            temp_transcript = df3.iloc[i]['transcript']\n",
    "            gen_wordcloud(temp_transcript, i)\n",
    "            tempdocvectors.append(doc_to_vect(temp_transcript))\n",
    "        \n",
    "        docvectors = pickle.load(open(path_models + \"docvectors.pickle\", 'rb'))\n",
    "        docvectors = docvectors+tempdocvectors\n",
    "        pickle.dump(docvectors, open(path_models + \"docvectors.pickle\",'wb'))\n",
    "        \n",
    "        similars = cosine_similarity(docvectors,docvectors).argsort()[:,-10:]\n",
    "        pickle.dump(similars, open(path_models + \"doc_similars.pickle\",'wb'))\n",
    "            \n",
    "        os.remove(path_temp+\"daiogenes.csv\")\n",
    "    \n",
    "    else:\n",
    "        return \"\"\"\n",
    "        <center><h1>ERROR!</h1> \"\"\"\n",
    "       \n",
    "\n",
    "    return redirect(url_for('library'))\n",
    "\n",
    "\n",
    "@app.route('/show')\n",
    "\n",
    "# show page: to show a transcription\n",
    "def show():  \n",
    "\n",
    "    show = request.args.get(\"video_id\", \"None\")\n",
    "    dfs= pd.read_csv(path+\"youtube_transcrip_vector.csv\") \n",
    "    i= int(show)\n",
    "    text = dfs.iloc[i]['transcript'].replace(\"\\n\",\"<br><br>\") \n",
    "    \n",
    "    return head(\"Transcription of: \"+str(dfs.iloc[i]['title'])) +\"<br><br><br><table Width=\\\"50%\\\"><tr>\"+ text+\"</tr></table>\"\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/admin')\n",
    "\n",
    "# admin page: to access admin menu (Cluster generation, topic generation and Update Database)\n",
    "def admin():\n",
    "    return \"\"\"\n",
    "    <center><table class=\"default\">\n",
    "    <td><center><a href=\\\"\"\"\"+ url_for('index')+\"\"\"\"\\\"><img src=\"\"\"+url_for('static', filename=\"Arts/CookAI_monster_mini.png\")+\"\"\"></center></td>\n",
    "    <td><center><h1>TrikAI Administration Panel</h1></center></td>\n",
    "    </table>\n",
    "    <br>   \n",
    "    \n",
    "    <table class=\"default\">\n",
    "    <tr>    \n",
    "    <th><center><h3><b>1.-</b><a href= \"\"\" +\"\\\"\"+ url_for('cluster')+\"\\\"\"+\"\"\"\">Cluster Generation</h3></center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <th><center><h3><b>2.-</b><a href= \"\"\" +\"\\\"\"+ url_for('topic')+\"\\\"\"+\"\"\"\">Topic Generation</h3></center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <th><center><h3><b>3.-</b><a href= \"\"\" +\"\\\"\"+ url_for('update')+\"\\\"\"+\"\"\"\">Update Database</h3></center></th>\n",
    "    </tr>\n",
    "    </table>\"\"\"\n",
    "   \n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "#debug = True\n",
    "    from waitress import serve\n",
    "    serve(app, host=\"0.0.0.0\", port=8081)\n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consult\t Flask\t KMeans\t Playlist\t STOPWORDS\t TfidfVectorizer\t Video_playlistForm\t WordCloud\t YouTubeTranscriptApi\t \n",
      "adjusted_rand_score\t centroids\t cluster_centroids\t cluster_keywords\t code\t counter\t daiogenes\t doc_to_vect\t document_size\t \n",
      "dtype\t file\t flash\t gen_wordcloud\t gen_wordcloud_temp\t get_playlist\t get_transcript\t head\t i\t \n",
      "import_video_data\t ktrain\t level\t load_topic_model\t meta\t metadata\t nlp\t nlp_cat\t os\t \n",
      "p\t p_list\t p_name\t path\t path_Wordcloud\t path_figures\t path_models\t path_temp\t pd\t \n",
      "pickle\t plt\t predict_level\t randint\t redirect\t render_table\t render_table_generic\t render_table_generic_links\t render_template\t \n",
      "request\t requests\t search\t sleep\t sns\t spacy\t topic_identification\t topics\t total\t \n",
      "transcripcion\t url\t url_for\t videos\t visualize_documents\t \n"
     ]
    }
   ],
   "source": [
    "%who"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
